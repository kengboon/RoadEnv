{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dim: 35 Action dim: 2 Action range: (-1.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "# Make environment\n",
    "# Working directory\n",
    "import sys, os\n",
    "if os.path.exists('road_env'):\n",
    "    sys.path.append('.')\n",
    "else:\n",
    "    sys.path.append('..')\n",
    "\n",
    "# Register environment\n",
    "from road_env import register_road_envs\n",
    "register_road_envs()\n",
    "\n",
    "# Make environment\n",
    "import gymnasium as gym\n",
    "env = gym.make('urban-road-v0', render_mode='rgb_array')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = env.action_space.high[0]\n",
    "print('State dim:', state_dim, 'Action dim:', action_dim, 'Action range:', (-max_action, max_action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Soft Q Network (1,2):  SoftQNetwork(\n",
      "  (linear1): Linear(in_features=37, out_features=512, bias=True)\n",
      "  (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (linear3): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (linear4): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "Policy Network:  PolicyNetwork(\n",
      "  (linear1): Linear(in_features=35, out_features=512, bias=True)\n",
      "  (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (linear3): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (linear4): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (mean_linear): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (log_std_linear): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Make DRL Agent\n",
    "hidden_dim = 512\n",
    "\n",
    "from rl_algorithms2.sac_v2 import SAC_Trainer, replay_buffer\n",
    "agent = SAC_Trainer(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    action_range=max_action,\n",
    "    hidden_dim=hidden_dim,\n",
    "    replay_buffer=replay_buffer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from ../../data/models/sac_v2-230704203226/9999\n"
     ]
    }
   ],
   "source": [
    "# Load trained model\n",
    "model_type = 'sac_v2'\n",
    "train_id = '230704203226'\n",
    "episode = '9999'\n",
    "model_dir = '../../data/models/' + model_type + '-' + train_id + '/' + episode\n",
    "agent.load_model(model_dir)\n",
    "print('Loaded from', model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from timeit\n",
    "class OCCLUSION(Enum):\n",
    "    Low = 1\n",
    "    Medium = 2\n",
    "    High = 3\n",
    "\n",
    "def run_test(env,\n",
    "             occlusion_level,\n",
    "             agent,\n",
    "             num_episode=20,\n",
    "             num_step=999,\n",
    "             auto_randseed=True,\n",
    "             render=False):\n",
    "    if isinstance(occlusion_level, OCCLUSION):\n",
    "        occlusion_level = occlusion_level._value_\n",
    "    env.configure({\n",
    "        'duration': num_step,\n",
    "        'obstacle_preset': occlusion_level\n",
    "    })\n",
    "\n",
    "    for episode in range(num_episode):\n",
    "        step = 0\n",
    "        episode_reward = 0\n",
    "\n",
    "        if auto_randseed:\n",
    "            env.configure({\n",
    "                'random_seed': int(episode * occlusion_level)\n",
    "            })\n",
    "        obs, info = env.reset()\n",
    "        while True: # Number of steps controlled by env.config['duration]\n",
    "            action = agent.policy_net.get_action(obs, deterministic=True)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            step += 1\n",
    "            episode_reward += reward\n",
    "            if done or truncated:\n",
    "                break\n",
    "        print(f'Episode: {episode+1}, Steps: {step}, Reward: {episode_reward:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "road-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
